{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTi09ZPCQeLgWFVHELSRAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashishsingh-26/Module8_Project/blob/main/Mod8_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install numpy pandas pdfreader\n",
        "import typing\n",
        "\n",
        "# Import libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from pdfreader import PDFDocument, SimplePDFViewer, document\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text: str):\n",
        "    \"\"\"\n",
        "    Given text it removes all the non-character words, small words,\n",
        "    converts everything to small letters, tokenizes and returns as a list.\n",
        "    :param text: The text to be cleaned\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-z]\", \" \", text)\n",
        "    data = text.split()\n",
        "    data = list(filter(lambda x: len(x) >= WORD_LENGTH_THRESHOLD, data))\n",
        "    return data\n",
        "\n",
        "# Function to parse text from PDF\n",
        "def parse_pdf(filename: str):\n",
        "    \"\"\"\n",
        "    Read text from a PDF file.\n",
        "    Clean the text, tokenize it, and return as a list of tokens.\n",
        "    :param :\n",
        "    \"\"\"\n",
        "    fd = open(filename, \"rb\")\n",
        "    document = PDFDocument(fd)\n",
        "    viewer = SimplePDFViewer(fd)\n",
        "    output_strings = []\n",
        "    for i in range(len(list(document.pages()))):\n",
        "        viewer.navigate(1)\n",
        "        viewer.render()\n",
        "        output_strings.extend(viewer.canvas.strings)\n",
        "    file_contents = \" \".join(output_strings)\n",
        "    return clean_text(file_contents)\n",
        "\n",
        "# Function to parse resume DataFrame\n",
        "def parse_resume_df():\n",
        "    resume_df = pd.read_csv(\"/content/resume-dataset.csv\")\n",
        "    resume_df[\"Keywords\"] = resume_df[\"Resume\"].apply(clean_text)\n",
        "    return resume_df[\"Keywords\"].values, resume_df[\"Category\"].values\n",
        "\n",
        "# Define classes\n",
        "\n",
        "# Bag of Words class\n",
        "class BagOfWords:\n",
        "    \"\"\"\n",
        "    A type of encoder, makes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: typing.Iterable) -> None:\n",
        "        \"\"\"\n",
        "        Generate the bag of words\n",
        "        :param data: an array of words, or an iterable containing arrays of words\n",
        "        \"\"\"\n",
        "        data = np.array(self.__linearize_array(data))\n",
        "        self.index_to_words = np.unique(data)\n",
        "        self.words_to_index = {w: i for i, w in enumerate(self.index_to_words)}\n",
        "\n",
        "    @classmethod\n",
        "    def __linearize_array(cls, text):\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                x.append(item)\n",
        "            else:\n",
        "                x.extend(cls.__linearize_array(item))\n",
        "        return x\n",
        "\n",
        "    def __call__(self, text: typing.Iterable[str]) -> np.array:\n",
        "        return self.get_counts(text)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.index_to_words)\n",
        "\n",
        "    def encode_data(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ) -> np.array:\n",
        "        \"\"\"\n",
        "        Compute the encodings of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                if item in self.words_to_index:\n",
        "                    x.append(self.words_to_index[item])\n",
        "            else:\n",
        "                x.append(self.encode_data(item))\n",
        "        return x\n",
        "\n",
        "    def decode_data(self: \"BagOfWords\", encoded_text: typing.Iterable[int]):\n",
        "        if isinstance(encoded_text, int) or isinstance(encoded_text, np.int64):\n",
        "            return self.index_to_words[encoded_text]\n",
        "        else:\n",
        "            return list(map(self.decode_data, encoded_text))\n",
        "\n",
        "    def get_counts(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the counts of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        if len(text) == 0 or isinstance(text[0], str):\n",
        "            x = np.zeros(shape=len(self))\n",
        "            for word in text:\n",
        "                if word in self.words_to_index:\n",
        "                    x[self.words_to_index[word]] += 1\n",
        "            return x\n",
        "        else:\n",
        "            return np.stack([self.get_counts(item) for item in text], axis=0)\n",
        "\n",
        "# Label Encoder class\n",
        "class LabelEncoder:\n",
        "    \"\"\"\n",
        "    Label encode a series of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data) -> None:\n",
        "        self.__training_data = data\n",
        "        self.index_to_token = list(set(data))\n",
        "        self.token_to_index = {\n",
        "            token: index for index, token in enumerate(self.index_to_token)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "\n",
        "    @property\n",
        "    def encoded_data(self):\n",
        "        return np.array([self.token_to_index[token] for token in self.__training_data])\n",
        "\n",
        "    def encode(self, data):\n",
        "        return np.array([self.token_to_index[token] for token in data])\n",
        "\n",
        "    def decode(self, data):\n",
        "        if isinstance(data, int) or isinstance(data, np.int64):\n",
        "            return self.index_to_token[data]\n",
        "        else:\n",
        "            return np.array([self.index_to_token[index] for index in data])\n",
        "\n",
        "# Bayesian Multiclass Model class\n",
        "class BayesianMulticlassModel:\n",
        "    \"\"\"\n",
        "    A multi-class bayesian classfier from encoded text tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, num_tokens) -> None:\n",
        "        self.counts = np.zeros(shape=(num_classes, num_tokens))\n",
        "\n",
        "    def fit(self, x_train: typing.Iterable[np.ndarray], y_train: typing.Iterable[int]):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "            self.counts[y] += x\n",
        "\n",
        "    def predict(self, counts_vector):\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "\n",
        "        likelihood = np.multiply(likelihood, counts_vector)\n",
        "        prior = np.expand_dims(prior, axis=1)\n",
        "\n",
        "        posterior_marginal = prior * likelihood / evidence + 0.00001\n",
        "        posterior_joint = np.sum(np.log(posterior_marginal), axis=1)\n",
        "        return np.flip(np.argsort(posterior_joint))\n",
        "\n",
        "# Bayesian Model Explainer class\n",
        "class BayesianModelExplainer(BayesianMulticlassModel):\n",
        "    \"\"\"\n",
        "    Explainer of the decision made by the base model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_encoder: LabelEncoder, bag_of_words: BagOfWords) -> None:\n",
        "        super().__init__(len(label_encoder), len(bag_of_words))\n",
        "        self.bag_of_words = bag_of_words\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def explain(self, text=None, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize what are the prior probabilities of classes and which words\n",
        "        add the the likelihood of each class.\n",
        "        \"\"\"\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "\n",
        "        if text is not None:\n",
        "            counts_vector = self.bag_of_words.get_counts(text)\n",
        "            likelihood = np.multiply(likelihood, counts_vector)\n",
        "\n",
        "        prior_ordering = np.flip(np.argsort(prior))\n",
        "        for item in prior_ordering:\n",
        "            likelihood = likelihood / (evidence + 0.00001)\n",
        "            label = self.label_encoder.decode(item)\n",
        "            word_ids = np.flip(np.argsort(likelihood[item]))\n",
        "            word_ids = word_ids[:10]\n",
        "            if label_filter is None or label in label_filter:\n",
        "                print(f\"{label}: {' '.join(self.bag_of_words.decode_data(word_ids))}\")\n",
        "\n",
        "# Main code\n",
        "\n",
        "# Set hyperparameters\n",
        "WORD_LENGTH_THRESHOLD = 3\n",
        "\n",
        "# Parse resume data\n",
        "x_train, y_train = parse_resume_df()\n",
        "\n",
        "# Create Bag of Words representation\n",
        "bag_of_words = BagOfWords(x_train)\n",
        "\n",
        "# Create Label Encoder\n",
        "label_encoder = LabelEncoder(y_train)\n",
        "\n",
        "# Encode training data\n",
        "x_train_encoded = bag_of_words.encode_data(x_train)\n",
        "y_train_encoded = label_encoder.encode(y_train)\n",
        "\n",
        "# Initialize and train Bayesian Multiclass Model\n",
        "model = BayesianMulticlassModel(len(label_encoder), len(bag_of_words))\n",
        "model.fit(x_train_encoded, y_train_encoded)\n",
        "\n",
        "# Parse PDF file\n",
        "pdf_filename = \"data/resumes/computers_2.pdf\"\n",
        "x_test = parse_pdf(pdf_filename)\n",
        "\n",
        "# Encode test data\n",
        "x_test_encoded = bag_of_words.get_counts(x_test)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(x_test_encoded)\n",
        "\n",
        "# Decode predictions\n",
        "decoded_predictions = label_encoder.decode(predictions)\n",
        "\n",
        "# Print top predicted job categories\n",
        "for job in decoded_predictions[:5]:\n",
        "    print(job)\n",
        "\n",
        "# Initialize and fit Bayesian Model Explainer\n",
        "explainable_model = BayesianModelExplainer(label_encoder, bag_of_words)\n",
        "explainable_model.fit(x_train=x_train_encoded, y_train=y_train_encoded)\n",
        "\n",
        "# Explain trained prior\n",
        "print(\"\\nANALYSIS OF TRAINED PRIOR\")\n",
        "print(\"-------------------------\")\n",
        "explainable_model.explain()\n",
        "\n",
        "# Explain trained evidence\n",
        "print(\"\\nANALYSIS OF TRAINED EVIDENCE\")\n",
        "print(\"----------------------------\")\n",
        "explainable_model.explain(x_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "oMNrW1tGZ7zq",
        "outputId": "304a693a-aae2-4b04-9b4c-e8ffe2f2a467"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: pdfreader in /usr/local/lib/python3.10/dist-packages (0.1.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: bitarray>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pdfreader) (2.9.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from pdfreader) (9.4.0)\n",
            "Requirement already satisfied: pycryptodome>=3.9.9 in /usr/local/lib/python3.10/dist-packages (from pdfreader) (3.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/resume-dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f54c4a13a54b>\u001b[0m in \u001b[0;36m<cell line: 225>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;31m# Parse resume data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_resume_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;31m# Create Bag of Words representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-f54c4a13a54b>\u001b[0m in \u001b[0;36mparse_resume_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Function to parse resume DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_resume_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresume_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/resume-dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mresume_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Keywords\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Resume\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Keywords\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/resume-dataset.csv'"
          ]
        }
      ]
    }
  ]
}